{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "\n",
    "project_root = os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Set GPU memory growth before initializing TensorFlow\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"GPU memory growth enabled for all GPUs.\")\n",
    "    except RuntimeError as e:\n",
    "        print(\"Failed to set memory growth:\", e)\n",
    "\n",
    "# Check TensorFlow version\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Check if GPU is available\n",
    "print(\"Is GPU available?\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# List all available devices\n",
    "print(\"\\nAvailable devices:\")\n",
    "for device in tf.config.list_physical_devices():\n",
    "    print(device)\n",
    "\n",
    "# Test if TensorFlow is using the GPU\n",
    "try:\n",
    "    with tf.device('/GPU:0'):\n",
    "        print(\"\\nRunning a simple computation on the GPU...\")\n",
    "        a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "        b = tf.constant([[1.0, 1.0], [0.0, 1.0]])\n",
    "        result = tf.matmul(a, b)\n",
    "        print(\"Matrix multiplication result:\\n\", result)\n",
    "except RuntimeError as e:\n",
    "    print(\"Error using GPU:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = (256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to load and preprocess images\n",
    "def load_image(image_path, target_size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Load an image, resize it, and preprocess it for neural network input.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "        target_size (tuple): Target size for the image, e.g., (128, 128).\n",
    "    \n",
    "    Returns:\n",
    "        tf.Tensor: Preprocessed image tensor.\n",
    "    \"\"\"\n",
    "    # Load the image\n",
    "    image = Image.open(image_path).convert('RGB')  # Ensure 3 channels (RGB)\n",
    "    \n",
    "    # Resize the image\n",
    "    image = image.resize(target_size, Image.LANCZOS)\n",
    "    \n",
    "    # Convert to a NumPy array and normalize pixel values\n",
    "    image_array = np.array(image) / 255.0  # Scale pixel values to [0, 1]\n",
    "    \n",
    "    # Convert to a TensorFlow tensor\n",
    "    image_tensor = tf.convert_to_tensor(image_array, dtype=tf.float32)\n",
    "    \n",
    "    # Add a batch dimension for processing in neural networks\n",
    "    image_tensor = tf.expand_dims(image_tensor, axis=0)  # Shape: (1, 128, 128, 3)\n",
    "    \n",
    "    return image_tensor\n",
    "\n",
    "# Display an image\n",
    "def display_image(image_tensor, title=\"Image\"):\n",
    "    \"\"\"\n",
    "    Display an image tensor.\n",
    "    \n",
    "    Args:\n",
    "        image_tensor (tf.Tensor): Image tensor with shape (1, height, width, channels).\n",
    "        title (str): Title of the plot.\n",
    "    \"\"\"\n",
    "    # Remove the batch dimension and clip pixel values\n",
    "    image = tf.squeeze(image_tensor, axis=0).numpy()  # Shape: (height, width, channels)\n",
    "    image = np.clip(image, 0, 1)  # Ensure pixel values are in the range [0, 1]\n",
    "    \n",
    "    # Plot the image\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Paths to your images\n",
    "style_image_path = os.path.join(project_root,\"data/optimization_method/starry_night.jpg\")  \n",
    "content_image_path = os.path.join(project_root, \"data/optimization_method/wedding_maya.jpg\")\n",
    "\n",
    "# Load and preprocess the images\n",
    "style_image = load_image(style_image_path, target_size=target_size)\n",
    "content_image = load_image(content_image_path, target_size=target_size)\n",
    "\n",
    "# Display the images\n",
    "display_image(style_image, title=\"Style Image\")\n",
    "display_image(content_image, title=\"Content Image\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "# Load a pre-trained VGG16 model\n",
    "vgg = VGG16(include_top=False, weights='imagenet', input_shape=target_size + (3,))\n",
    "\n",
    "for layer in vgg.layers:\n",
    "    layer.trainable = False # Freeze the layers\n",
    "    \n",
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_extractor(pre_trained_model: tf.keras.Model, layer_names: list) -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Build a feature extractor model that returns intermediate layer outputs.\n",
    "    \n",
    "    Args:\n",
    "        model (tf.keras.Model): The pretrained model to use as a feature extractor.\n",
    "        layer_names (list): Names of the layers to use for feature extraction.\n",
    "    \n",
    "    Returns:\n",
    "        tf.keras.Model: The feature extractor model.\n",
    "    \"\"\"\n",
    "    # Get the intermediate layer outputs\n",
    "    outputs = [pre_trained_model.get_layer(name).output for name in layer_names]\n",
    "    \n",
    "    # Build the feature extractor model\n",
    "    return tf.keras.Model(inputs=pre_trained_model.input, outputs=outputs)\n",
    "\n",
    "# Define the content and style layer names\n",
    "content_layers = [\"block4_conv2\"]\n",
    "style_layers = [ \"block1_conv1\", \"block2_conv1\", \"block3_conv1\", \"block4_conv1\"]\n",
    "\n",
    "# Instantiate feature extractor\n",
    "feature_extractor = get_feature_extractor(vgg, content_layers + style_layers)\n",
    "\n",
    "\n",
    "# Extract activation for given image\n",
    "def get_activations(image, feature_extractor: tf.keras.Model) -> tuple:\n",
    "    \"\"\"\n",
    "    Forward pass to get the activations of the content and style layers for an image.\n",
    "    \n",
    "    Args:\n",
    "        image (tf.Tensor): Input image tensor.\n",
    "        feature_extractor (tf.keras.Model): Feature extractor model.\n",
    "    \"\"\"\n",
    "    \n",
    "    preprocessed_image = tf.keras.applications.vgg16.preprocess_input(image * 255) # Useful for VGG\n",
    "    \n",
    "    # Get activations\n",
    "    activations = feature_extractor(preprocessed_image)\n",
    "    content_activations = activations[:len(content_layers)]\n",
    "    style_activations = activations[len(content_layers):]\n",
    "    \n",
    "    del activations\n",
    "    \n",
    "    return content_activations, style_activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content Loss\n",
    "def content_loss(content_activation: list[tf.Tensor], target_activation: list[tf.Tensor]) -> tf.Tensor:\n",
    "    sum = 0\n",
    "    n_elements = 0\n",
    "    for c, t in zip(content_activation, target_activation):\n",
    "        assert c.shape == t.shape, \"Activations are different shape.\"\n",
    "        sum += tf.math.reduce_sum(tf.math.square(c - t))\n",
    "        n_elements += tf.size(c)\n",
    "    \n",
    "    return sum / tf.cast(n_elements, tf.float32)\n",
    "    \n",
    "\n",
    "# Style loss\n",
    "def gram_matrix(activation: tf.Tensor) -> tf.Tensor:\n",
    "    # Flatten\n",
    "    transpose = tf.transpose(activation, perm=[0, 3, 1, 2])\n",
    "    shape = tf.shape(transpose)\n",
    "    reshaped = tf.reshape(transpose, [shape[0], shape[1], -1])\n",
    "    \n",
    "    return tf.matmul(reshaped, tf.transpose(reshaped, perm=[0, 2, 1]))\n",
    "\n",
    "\n",
    "def style_loss(style_activations: list[tf.Tensor], target_activations: list[tf.Tensor]) -> tf.Tensor:\n",
    "    # Gram matrices\n",
    "    style_grams = [gram_matrix(style) for style in style_activations]\n",
    "    target_grams = [gram_matrix(target) for target in target_activations]\n",
    "    \n",
    "    # MSE\n",
    "    sum = 0\n",
    "    n_elements = 0\n",
    "    \n",
    "    for m, n in zip(style_grams, target_grams):\n",
    "        assert m.shape == n.shape, \"Gram matrices are different shape.\"\n",
    "        sum += tf.math.reduce_sum(tf.math.square(m - n))\n",
    "        n_elements += tf.size(m)\n",
    "        \n",
    "    return sum / tf.cast(n_elements, tf.float32)\n",
    "\n",
    "# Total variation loss\n",
    "def total_loss(content_loss, style_loss, weights: dict) -> tf.Tensor:\n",
    "    return weights['content'] * content_loss + weights['style'] * style_loss\n",
    "\n",
    "weights = {\n",
    "\t'content' : 1,\n",
    "\t'style' : 1e3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save target image\n",
    "# Function to save target image\n",
    "def save_image(image_tensor, epoch, method=\"optimization_method\", version=\"test\"):\n",
    "    \"\"\"\n",
    "    Save an image tensor to a file.\n",
    "    \n",
    "    Args:\n",
    "        image_tensor (tf.Tensor): Image tensor with shape (1, height, width, channels).\n",
    "        epoch (int): Current epoch number.\n",
    "        method (str): The optimization method used.\n",
    "        version (str): The version or configuration identifier.\n",
    "    \"\"\"\n",
    "    image = tf.squeeze(image_tensor, axis=0).numpy()  # Remove batch dimension\n",
    "    image = np.clip(image, 0, 1)\n",
    "    image = tf.image.convert_image_dtype(image, tf.uint8)\n",
    "    \n",
    "    # Define the directory and ensure it exists\n",
    "    save_dir = os.path.join(project_root, f\"models/ouputs_monitoring/{method}/{version}\")\n",
    "    os.makedirs(save_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "    \n",
    "    # Construct file path\n",
    "    file_path = os.path.join(save_dir, f\"output_image_{epoch}.png\")\n",
    "    \n",
    "    # Save the image\n",
    "    tf.keras.utils.save_img(file_path, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "# Variable\n",
    "target_image = tf.Variable(tf.identity(content_image), trainable=True) # Copy of content image\n",
    "\n",
    "# Constants\n",
    "feature_extractor = get_feature_extractor(vgg, content_layers + style_layers)\n",
    "\n",
    "content_activations, _ = get_activations(content_image, feature_extractor)\n",
    "_, style_activations = get_activations(style_image, feature_extractor)\n",
    "\n",
    "weights = {'content' : 1, 'style' : 1e4} # Loss weights\n",
    "\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-3, beta_1=0.9, beta_2=0.99, epsilon=1e-7)\n",
    "\n",
    "#@tf.function\n",
    "def train_step(target_image, content_activations: list[tf.Tensor], style_activations: list[tf.Tensor], weights: dict, \n",
    "               optimizer: tf.keras.optimizers.Optimizer, feature_extractor: tf.keras.Model):\n",
    "    \n",
    "    # record forward pass\n",
    "    with tf.GradientTape() as tape:\n",
    "        target_content, target_style = get_activations(target_image, feature_extractor)\n",
    "        \n",
    "        c_loss = content_loss(content_activations, target_content)\n",
    "        s_loss = style_loss(style_activations, target_style)\n",
    "        loss = total_loss(c_loss, s_loss, weights)\n",
    "        \n",
    "    # compute gradients\n",
    "    gradients = tape.gradient(loss, target_image)\n",
    "    \n",
    "    # update weights\n",
    "    optimizer.apply_gradients(zip([gradients], [target_image]))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 1501\n",
    "for epoch in tqdm(range(n_epochs), desc=\"Image optimization\"):\n",
    "    loss = train_step(target_image, content_activations, style_activations, weights, optimizer, feature_extractor)\n",
    "    \n",
    "    if epoch % 250 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss: {loss}\")\n",
    "        clipped_image = tf.clip_by_value(target_image, 0., 1.)\n",
    "        # Save image\n",
    "        save_image(clipped_image, epoch, method=\"optimization_method\", version=\"maya\")\n",
    "        \n",
    "        # Display image\n",
    "        clear_output(wait=True)\n",
    "        display_image(clipped_image, title=f\"Epoch {epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_monitoring_files(method=\"optimization_method\", version=\"test\"):\n",
    "    import shutil\n",
    "    path = os.path.join(project_root, f\"models/ouputs_monitoring/{method}/{version}\")\n",
    "    shutil.rmtree(path)\n",
    "    os.makedirs(path)\n",
    "    \n",
    "clear_monitoring_files(version='maya2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colorimetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the generated image\n",
    "gen_im_path = os.path.join(project_root, \"models/ouputs_monitoring/optimization_method/maya/output_image_750.png\")\n",
    "generated_image = load_image(gen_im_path, target_size=target_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1 - Histogram Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.exposure import match_histograms\n",
    "\n",
    "def match_colors(generated_image, content_image):\n",
    "    \"\"\"\n",
    "    Match the color distribution of the generated image to the content image.\n",
    "\n",
    "    Args:\n",
    "        generated_image (np.ndarray): Generated image as a NumPy array.\n",
    "        content_image (np.ndarray): Content image as a NumPy array.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Color-transferred image.\n",
    "    \"\"\"\n",
    "    matched_image = match_histograms(generated_image, content_image, channel_axis=-1)\n",
    "    return matched_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_image = match_colors(generated_image.numpy(), content_image.numpy())\n",
    "\n",
    "# Display the images\n",
    "display_image(content_image, title=\"Content Image\")\n",
    "display_image(generated_image, title=\"Generated Image\")\n",
    "display_image(matched_image, title=\"Color-Transferred Image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2 - Std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_mean_std(generated_image, content_image):\n",
    "    \"\"\"\n",
    "    Adjust the mean and standard deviation of the generated image to match the content image.\n",
    "\n",
    "    Args:\n",
    "        generated_image (np.ndarray): Generated image as a NumPy array.\n",
    "        content_image (np.ndarray): Content image as a NumPy array.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Color-matched image.\n",
    "    \"\"\"\n",
    "    for channel in range(3):  # Assuming RGB\n",
    "        gen_mean, gen_std = generated_image[..., channel].mean(), generated_image[..., channel].std()\n",
    "        cont_mean, cont_std = content_image[..., channel].mean(), content_image[..., channel].std()\n",
    "        generated_image[..., channel] = (\n",
    "            (generated_image[..., channel] - gen_mean) / (gen_std + 1e-8)\n",
    "        ) * cont_std + cont_mean\n",
    "    return np.clip(generated_image, 0, 1)  # Ensure valid pixel range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_image = match_mean_std(generated_image.numpy(), content_image.numpy())\n",
    "\n",
    "# Display the images\n",
    "display_image(content_image, title=\"Content Image\")\n",
    "display_image(generated_image, title=\"Generated Image\")\n",
    "display_image(matched_image, title=\"Color-Transferred Image\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
